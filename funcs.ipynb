{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b7fc2c4-705e-4170-8549-00fd78deac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98a1b785-4b31-4c9c-b7dc-fb37b935c25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv(file_path):\n",
    "    # Read the CSV file into a pandas dataframe\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Handling missing values\n",
    "    # Replace any missing values (NaN) with appropriate strategies\n",
    "    # For numerical columns, you can use methods like mean, median, or interpolation\n",
    "    df.fillna(df.mean(), inplace=True)  # Example: Replace missing values with column means\n",
    "    \n",
    "    # For categorical columns, you can use methods like mode or a constant value\n",
    "    df.fillna('Unknown', inplace=True)  # Example: Replace missing values with 'Unknown'\n",
    "    \n",
    "    # Process or remodel the description columns\n",
    "    # Depending on your specific use case, you can apply techniques like text preprocessing, feature extraction, or encoding\n",
    "    \n",
    "    # Text preprocessing: Remove special characters, convert to lowercase, etc.\n",
    "    #df['description'] = df['description'].str.replace('[^\\w\\s]', '').str.lower()\n",
    "    \n",
    "    # Feature extraction: Extract relevant information from the description\n",
    "    # You can use techniques like TF-IDF, word embeddings, or topic modeling to extract features\n",
    "    \n",
    "    # Encoding: Convert categorical description columns into numeric representations\n",
    "    # Techniques like one-hot encoding or word embeddings can be useful\n",
    "    \n",
    "    # Return the preprocessed dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df8feece-0f63-46c1-9c50-dc1d7b6aa852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnLanguage(file, doc2vec):\n",
    "    \n",
    "    \n",
    "    data = pd.read_csv(file)\n",
    "    X = pd.read_csv(doc2vec, index_col = False, delimiter = ',', header = None)\n",
    "    y = data[\"Language\"]\n",
    "    \n",
    "    y = pd.Series(y)\n",
    "    # Assuming you have a feature matrix `X` and a target variable `y`\n",
    "    # X should contain other features like doc2vec and word frequency counts\n",
    "    # y should contain the language labels (with missing values)\n",
    "\n",
    "    # Split the dataset into instances with and without missing language values\n",
    "    X_with_language = X[~y.isnull()]\n",
    "    y_with_language = y[~y.isnull()]\n",
    "    X_missing_language = X[y.isnull()]\n",
    "\n",
    "    # Split the dataset with language into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_with_language, y_with_language, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a Random Forest classifier on the instances with language\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the missing language values using the trained model\n",
    "    imputed_language = rf.predict(X_missing_language)\n",
    "\n",
    "    # Merge the imputed language values with the original dataset\n",
    "    y[y.isnull()] = imputed_language\n",
    "\n",
    "    # Now you can proceed with your machine learning algorithm using the complete dataset\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50557e2f-a788-4217-b957-e16067a3ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, test_size):\n",
    "    # Separate the features and target variable\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Return the split datasets\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "316a35f4-9420-4c20-b66c-b39295cf50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(test_df, results_series):\n",
    "    successful_count = 0\n",
    "    for test, result in zip(test_df.iterrows(), results_series.iteritems()):\n",
    "        if test[-1][-1] == result[1]:\n",
    "            successful_count += 1\n",
    "    if (len(test_df) != 0):\n",
    "        return successful_count / (len(test_df))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc38ca34-719c-41f6-999b-9c17b92bbd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(test_df, results_series, positive_label):\n",
    "    tp_count = 0\n",
    "    fp_count = 0\n",
    "    for test, result in zip(test_df.iterrows(), results_series.iteritems()):\n",
    "        if result[1] == positive_label:\n",
    "            if test[-1][-1] == result[1]:\n",
    "                tp_count += 1\n",
    "            else:\n",
    "                fp_count += 1\n",
    "    if (tp_count + fp_count != 0):\n",
    "        return tp_count / (tp_count + fp_count)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "afeb30db-1891-43c6-ab27-889c35eb8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(test_df, results_series, positive_label):\n",
    "    tp_count = 0\n",
    "    fn_count = 0\n",
    "    for test, result in zip(test_df.iterrows(), results_series.iteritems()):\n",
    "        if test[-1][-1] == positive_label:\n",
    "            if test[-1][-1] == result[1]:\n",
    "                tp_count += 1\n",
    "            else:\n",
    "                fn_count += 1\n",
    "    if (tp_count + fn_count != 0):\n",
    "        return tp_count / (tp_count + fn_count)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5fcab34-fcf1-4d99-bf85-d9b67e83741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_letter_to_cols(df, letter):\n",
    "    for column in df.columns:\n",
    "        new_name = letter + str(column)\n",
    "        df.rename(columns={column: new_name}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af0285f6-484b-4c69-8527-c1c498ef0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lassoFeatures(data):\n",
    "    \n",
    "    target = data.pop(\"rating_label\")\n",
    "\n",
    "    # Assuming your features are stored in a pandas DataFrame called 'data'\n",
    "    # and your target variable is stored in a pandas Series called 'target'\n",
    "\n",
    "    # Split the data into training, validation, and testing sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Initialize a list to store the performance metrics\n",
    "    mse_scores = []\n",
    "\n",
    "    # Define a range of alpha values to try\n",
    "    alpha_values = [0.1, 0.3, 0.5, 0.8, 1.0, 1.5]\n",
    "\n",
    "    # Iterate over the alpha values\n",
    "    for alpha in alpha_values:\n",
    "        # Initialize and fit the Lasso model\n",
    "        lasso = Lasso(alpha=alpha)\n",
    "        lasso.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the validation set\n",
    "        y_pred = lasso.predict(X_val)\n",
    "\n",
    "        # Calculate the mean squared error (MSE)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mse_scores.append(mse)\n",
    "\n",
    "    # Find the alpha value with the lowest MSE\n",
    "    best_alpha = alpha_values[mse_scores.index(min(mse_scores))]\n",
    "\n",
    "    # Train the final model with the best alpha value on the combined training and validation sets\n",
    "    X_train_val = pd.concat([X_train, X_val])\n",
    "    y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "    lasso = Lasso(alpha=best_alpha)\n",
    "    lasso.fit(X_train_val, y_train_val)\n",
    "\n",
    "    # Get the coefficients\n",
    "    lasso_coef = lasso.coef_\n",
    "\n",
    "    # Print the feature importances\n",
    "    for feature, coef in zip(data.columns, lasso_coef):\n",
    "        print(f\"{feature}: {coef}\")\n",
    "        \n",
    "    \n",
    "    return lasso_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a2c6b-079a-45ef-af9f-77d4bbd6407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that converts a categorical feature into nominal, while maintaing categorical properties\n",
    "def numericise_categorical_data(df, feature):\n",
    "    df[feature] = df[feature].astype(str)\n",
    "    df[feature] = df[feature].astype('category')\n",
    "    df_encoded = pd.get_dummies(df, columns=[feature])\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    df[feature] = encoder.fit_transform(df[feature])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f3c1e5-6c20-4bac-b190-4ea94bf9c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimFeatures(filename, k = 10):\n",
    "    \n",
    "    #k is how many features to select\n",
    "    \n",
    "    data = pd.read_csv(filename)\n",
    "    \n",
    "    lasso_coef = lassoFeatures(data)\n",
    "    \n",
    "    # Getting magnitudes of coefficients, ranking them\n",
    "    \n",
    "    lasso_coef_abs = abs(lasso_coef)\n",
    "    feature_ranking = sorted(range(len(lasso_coef_abs)), key=lambda k: lasso_coef_abs[k], reverse=True)\n",
    "    \n",
    "    \n",
    "    #selecting most influential features, returning as list of names\n",
    "    selected_features = data.columns[feature_ranking[:k]]\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b3bea6d-ab50-422c-a4d7-46925fd7a079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leoro\\AppData\\Local\\Temp\\ipykernel_17316\\2049216653.py:8: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df.fillna(df.mean(), inplace=True)  # Example: Replace missing values with column means\n"
     ]
    }
   ],
   "source": [
    "df = preprocess_csv('project_data_files/book_rating_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92cb17ef-b221-42f4-a6ef-917b96b5f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_name_features = pd.read_csv(r\"project_data_files/book_text_features_doc2vec/train_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "add_letter_to_cols(book_name_features, \"n\")\n",
    "\n",
    "book_desc_features = pd.read_csv(r\"project_data_files/book_text_features_doc2vec/train_desc_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "add_letter_to_cols(book_desc_features, \"d\")\n",
    "\n",
    "book_auth_features = pd.read_csv(r\"project_data_files/book_text_features_doc2vec/train_authors_doc2vec20.csv\", index_col = False, delimiter = ',', header=None)\n",
    "add_letter_to_cols(book_auth_features, \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5993b31-f1f3-4671-b750-a7a1263a8296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n0</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>n3</th>\n",
       "      <th>n4</th>\n",
       "      <th>n5</th>\n",
       "      <th>n6</th>\n",
       "      <th>n7</th>\n",
       "      <th>n8</th>\n",
       "      <th>n9</th>\n",
       "      <th>...</th>\n",
       "      <th>d91</th>\n",
       "      <th>d92</th>\n",
       "      <th>d93</th>\n",
       "      <th>d94</th>\n",
       "      <th>d95</th>\n",
       "      <th>d96</th>\n",
       "      <th>d97</th>\n",
       "      <th>d98</th>\n",
       "      <th>d99</th>\n",
       "      <th>rating_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052262</td>\n",
       "      <td>-0.263308</td>\n",
       "      <td>0.026872</td>\n",
       "      <td>0.128574</td>\n",
       "      <td>-0.161565</td>\n",
       "      <td>-0.127520</td>\n",
       "      <td>0.249588</td>\n",
       "      <td>0.037621</td>\n",
       "      <td>-0.074043</td>\n",
       "      <td>0.072854</td>\n",
       "      <td>...</td>\n",
       "      <td>1.096503</td>\n",
       "      <td>0.894538</td>\n",
       "      <td>-0.386222</td>\n",
       "      <td>1.000658</td>\n",
       "      <td>1.094646</td>\n",
       "      <td>-0.897948</td>\n",
       "      <td>0.256250</td>\n",
       "      <td>-0.743381</td>\n",
       "      <td>-0.046537</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.129112</td>\n",
       "      <td>0.021312</td>\n",
       "      <td>0.159166</td>\n",
       "      <td>-0.072448</td>\n",
       "      <td>0.036028</td>\n",
       "      <td>-0.093721</td>\n",
       "      <td>0.129199</td>\n",
       "      <td>0.069736</td>\n",
       "      <td>-0.253263</td>\n",
       "      <td>-0.066424</td>\n",
       "      <td>...</td>\n",
       "      <td>2.018345</td>\n",
       "      <td>-0.515164</td>\n",
       "      <td>0.510041</td>\n",
       "      <td>1.042953</td>\n",
       "      <td>0.034085</td>\n",
       "      <td>0.397630</td>\n",
       "      <td>0.180119</td>\n",
       "      <td>-0.133072</td>\n",
       "      <td>1.251777</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170058</td>\n",
       "      <td>0.052351</td>\n",
       "      <td>-0.013406</td>\n",
       "      <td>0.099001</td>\n",
       "      <td>0.083173</td>\n",
       "      <td>-0.161439</td>\n",
       "      <td>0.048635</td>\n",
       "      <td>0.089419</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>-0.063164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043291</td>\n",
       "      <td>0.166269</td>\n",
       "      <td>0.443516</td>\n",
       "      <td>0.360877</td>\n",
       "      <td>0.637700</td>\n",
       "      <td>-0.399422</td>\n",
       "      <td>-0.217829</td>\n",
       "      <td>0.095041</td>\n",
       "      <td>0.030425</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.250849</td>\n",
       "      <td>0.021555</td>\n",
       "      <td>0.091047</td>\n",
       "      <td>-0.041589</td>\n",
       "      <td>-0.040949</td>\n",
       "      <td>0.240260</td>\n",
       "      <td>0.415056</td>\n",
       "      <td>0.027029</td>\n",
       "      <td>-0.172413</td>\n",
       "      <td>-0.135485</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.064901</td>\n",
       "      <td>0.956356</td>\n",
       "      <td>0.537667</td>\n",
       "      <td>-1.156633</td>\n",
       "      <td>1.138308</td>\n",
       "      <td>0.287945</td>\n",
       "      <td>0.809811</td>\n",
       "      <td>-1.180691</td>\n",
       "      <td>-0.075178</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.041681</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>-0.051164</td>\n",
       "      <td>-0.076813</td>\n",
       "      <td>0.096855</td>\n",
       "      <td>-0.215943</td>\n",
       "      <td>0.152729</td>\n",
       "      <td>0.267636</td>\n",
       "      <td>-0.079954</td>\n",
       "      <td>-0.065560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.730932</td>\n",
       "      <td>-0.893566</td>\n",
       "      <td>0.982820</td>\n",
       "      <td>0.190981</td>\n",
       "      <td>0.605344</td>\n",
       "      <td>0.236092</td>\n",
       "      <td>0.653281</td>\n",
       "      <td>-0.581590</td>\n",
       "      <td>-0.850868</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23058</th>\n",
       "      <td>0.007497</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.019723</td>\n",
       "      <td>-0.003321</td>\n",
       "      <td>0.021097</td>\n",
       "      <td>-0.129420</td>\n",
       "      <td>0.130302</td>\n",
       "      <td>-0.037361</td>\n",
       "      <td>-0.004281</td>\n",
       "      <td>-0.255112</td>\n",
       "      <td>...</td>\n",
       "      <td>2.021390</td>\n",
       "      <td>0.418629</td>\n",
       "      <td>-0.371224</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.869552</td>\n",
       "      <td>-3.437345</td>\n",
       "      <td>1.491958</td>\n",
       "      <td>2.093727</td>\n",
       "      <td>1.478695</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23059</th>\n",
       "      <td>-0.024484</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>-0.015977</td>\n",
       "      <td>0.086630</td>\n",
       "      <td>0.082127</td>\n",
       "      <td>-0.174537</td>\n",
       "      <td>0.011694</td>\n",
       "      <td>0.111608</td>\n",
       "      <td>-0.106961</td>\n",
       "      <td>-0.147956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234319</td>\n",
       "      <td>0.114523</td>\n",
       "      <td>0.223425</td>\n",
       "      <td>0.818674</td>\n",
       "      <td>0.719629</td>\n",
       "      <td>-1.334342</td>\n",
       "      <td>-1.144812</td>\n",
       "      <td>-0.270687</td>\n",
       "      <td>-1.546596</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23060</th>\n",
       "      <td>-0.099309</td>\n",
       "      <td>-0.046230</td>\n",
       "      <td>-0.033294</td>\n",
       "      <td>0.242591</td>\n",
       "      <td>-0.055477</td>\n",
       "      <td>-0.033886</td>\n",
       "      <td>0.026869</td>\n",
       "      <td>0.038410</td>\n",
       "      <td>-0.126636</td>\n",
       "      <td>0.127742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.308627</td>\n",
       "      <td>-0.630947</td>\n",
       "      <td>-0.264485</td>\n",
       "      <td>0.316840</td>\n",
       "      <td>0.305589</td>\n",
       "      <td>-0.123598</td>\n",
       "      <td>-0.424452</td>\n",
       "      <td>-1.336598</td>\n",
       "      <td>0.163445</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23061</th>\n",
       "      <td>-0.038388</td>\n",
       "      <td>0.065679</td>\n",
       "      <td>-0.159324</td>\n",
       "      <td>-0.048682</td>\n",
       "      <td>0.054175</td>\n",
       "      <td>0.317751</td>\n",
       "      <td>0.065931</td>\n",
       "      <td>-0.126021</td>\n",
       "      <td>-0.105057</td>\n",
       "      <td>-0.147185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085357</td>\n",
       "      <td>-0.113327</td>\n",
       "      <td>1.173376</td>\n",
       "      <td>1.244604</td>\n",
       "      <td>1.042439</td>\n",
       "      <td>-0.130578</td>\n",
       "      <td>0.552256</td>\n",
       "      <td>1.143148</td>\n",
       "      <td>-0.685621</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23062</th>\n",
       "      <td>-0.051934</td>\n",
       "      <td>-0.005339</td>\n",
       "      <td>0.030417</td>\n",
       "      <td>0.044278</td>\n",
       "      <td>-0.031339</td>\n",
       "      <td>-0.020412</td>\n",
       "      <td>0.038031</td>\n",
       "      <td>0.073363</td>\n",
       "      <td>-0.068429</td>\n",
       "      <td>-0.037205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.284325</td>\n",
       "      <td>-0.456627</td>\n",
       "      <td>0.377431</td>\n",
       "      <td>0.335774</td>\n",
       "      <td>0.535361</td>\n",
       "      <td>-0.383517</td>\n",
       "      <td>0.215218</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>-0.013714</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23063 rows × 221 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             n0        n1        n2        n3        n4        n5        n6  \\\n",
       "0      0.052262 -0.263308  0.026872  0.128574 -0.161565 -0.127520  0.249588   \n",
       "1     -0.129112  0.021312  0.159166 -0.072448  0.036028 -0.093721  0.129199   \n",
       "2     -0.170058  0.052351 -0.013406  0.099001  0.083173 -0.161439  0.048635   \n",
       "3      0.250849  0.021555  0.091047 -0.041589 -0.040949  0.240260  0.415056   \n",
       "4     -0.041681  0.038051 -0.051164 -0.076813  0.096855 -0.215943  0.152729   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "23058  0.007497  0.000220  0.019723 -0.003321  0.021097 -0.129420  0.130302   \n",
       "23059 -0.024484  0.000467 -0.015977  0.086630  0.082127 -0.174537  0.011694   \n",
       "23060 -0.099309 -0.046230 -0.033294  0.242591 -0.055477 -0.033886  0.026869   \n",
       "23061 -0.038388  0.065679 -0.159324 -0.048682  0.054175  0.317751  0.065931   \n",
       "23062 -0.051934 -0.005339  0.030417  0.044278 -0.031339 -0.020412  0.038031   \n",
       "\n",
       "             n7        n8        n9  ...       d91       d92       d93  \\\n",
       "0      0.037621 -0.074043  0.072854  ...  1.096503  0.894538 -0.386222   \n",
       "1      0.069736 -0.253263 -0.066424  ...  2.018345 -0.515164  0.510041   \n",
       "2      0.089419 -0.072266 -0.063164  ... -0.043291  0.166269  0.443516   \n",
       "3      0.027029 -0.172413 -0.135485  ... -1.064901  0.956356  0.537667   \n",
       "4      0.267636 -0.079954 -0.065560  ... -0.730932 -0.893566  0.982820   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "23058 -0.037361 -0.004281 -0.255112  ...  2.021390  0.418629 -0.371224   \n",
       "23059  0.111608 -0.106961 -0.147956  ...  0.234319  0.114523  0.223425   \n",
       "23060  0.038410 -0.126636  0.127742  ... -0.308627 -0.630947 -0.264485   \n",
       "23061 -0.126021 -0.105057 -0.147185  ... -0.085357 -0.113327  1.173376   \n",
       "23062  0.073363 -0.068429 -0.037205  ... -0.284325 -0.456627  0.377431   \n",
       "\n",
       "            d94       d95       d96       d97       d98       d99  \\\n",
       "0      1.000658  1.094646 -0.897948  0.256250 -0.743381 -0.046537   \n",
       "1      1.042953  0.034085  0.397630  0.180119 -0.133072  1.251777   \n",
       "2      0.360877  0.637700 -0.399422 -0.217829  0.095041  0.030425   \n",
       "3     -1.156633  1.138308  0.287945  0.809811 -1.180691 -0.075178   \n",
       "4      0.190981  0.605344  0.236092  0.653281 -0.581590 -0.850868   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "23058  0.595000  0.869552 -3.437345  1.491958  2.093727  1.478695   \n",
       "23059  0.818674  0.719629 -1.334342 -1.144812 -0.270687 -1.546596   \n",
       "23060  0.316840  0.305589 -0.123598 -0.424452 -1.336598  0.163445   \n",
       "23061  1.244604  1.042439 -0.130578  0.552256  1.143148 -0.685621   \n",
       "23062  0.335774  0.535361 -0.383517  0.215218  0.000406 -0.013714   \n",
       "\n",
       "       rating_label  \n",
       "0               4.0  \n",
       "1               4.0  \n",
       "2               4.0  \n",
       "3               4.0  \n",
       "4               3.0  \n",
       "...             ...  \n",
       "23058           4.0  \n",
       "23059           4.0  \n",
       "23060           4.0  \n",
       "23061           4.0  \n",
       "23062           4.0  \n",
       "\n",
       "[23063 rows x 221 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform label encoding for publishers, language\n",
    "label_encoder = LabelEncoder()\n",
    "#combined_df['Publisher'] = label_encoder.fit_transform(combined_df['Publisher'])\n",
    "#combined_df['Language'] = label_encoder.fit_transform(combined_df['Language'])\n",
    "\n",
    "combined_df = pd.concat([book_name_features, book_auth_features, book_desc_features, df['rating_label']], axis=1)\n",
    "# Separate the feature columns (X) and the target column (y)\n",
    "X = combined_df.drop('rating_label', axis=1)\n",
    "y = combined_df['rating_label']\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22fee49e-684a-4caf-894d-bda64bc1b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features and Mutual Information Scores (in descending order):\n",
      "n43: 0.010703697516897481\n",
      "d52: 0.008796306528973119\n",
      "a16: 0.00816979642552873\n",
      "n48: 0.008140423252084306\n",
      "n18: 0.008108361674596054\n",
      "n58: 0.007817684898878019\n",
      "d66: 0.007802212338136938\n",
      "n40: 0.007648576017754705\n",
      "n34: 0.007402328585371265\n",
      "d87: 0.00736629403527389\n",
      "n98: 0.007159293661197941\n",
      "d48: 0.007061555202364067\n",
      "n85: 0.006960020449693793\n",
      "n32: 0.006732499874498643\n",
      "d79: 0.006603224790930007\n",
      "d92: 0.006433214095819428\n",
      "n45: 0.006418479079567607\n",
      "n5: 0.006376844159886819\n",
      "n49: 0.0060596660680192205\n",
      "d77: 0.005872029034257764\n",
      "d96: 0.005864585576081716\n",
      "n31: 0.005797234744699065\n",
      "d15: 0.005670658097733927\n",
      "n70: 0.005657849195449094\n",
      "d62: 0.005645248450387497\n",
      "d12: 0.005629982611626172\n",
      "d86: 0.005541854379304878\n",
      "n23: 0.005504472261010029\n",
      "n44: 0.005499185056922595\n",
      "n50: 0.005408631943596864\n",
      "d17: 0.0053894134018588336\n",
      "d45: 0.00537954487022918\n",
      "n59: 0.005365338238702444\n",
      "n20: 0.005294631299063379\n",
      "d24: 0.005080024818401974\n",
      "n27: 0.0049515097693861865\n",
      "n41: 0.0048755002752498555\n",
      "n63: 0.004841455122585048\n",
      "d9: 0.004811976027394582\n",
      "d46: 0.0047257971574272695\n",
      "a6: 0.00464347877623239\n",
      "n55: 0.004642218849945978\n",
      "d88: 0.00456767051579865\n",
      "n81: 0.004450983799068053\n",
      "n95: 0.004379280075388792\n",
      "d53: 0.004335888786512365\n",
      "n38: 0.004254401727969315\n",
      "n25: 0.004253797461037712\n",
      "n8: 0.004189211295733353\n",
      "d44: 0.00415035467709135\n",
      "n88: 0.0041362869644432365\n",
      "d33: 0.0041250716034062496\n",
      "n91: 0.004123982591696551\n",
      "d55: 0.0040066303043788665\n",
      "n4: 0.003833889107665245\n",
      "n72: 0.0038260492753501296\n",
      "d5: 0.0038218155282103705\n",
      "d34: 0.003803759101331128\n",
      "n14: 0.0037691353014612794\n",
      "d43: 0.003766327731767216\n",
      "d2: 0.003671021944690045\n",
      "n78: 0.0036389720365486156\n",
      "d80: 0.0036208167715903183\n",
      "d31: 0.0036106813167386953\n",
      "n96: 0.003605251278295807\n",
      "n60: 0.003568047564539656\n",
      "n33: 0.003480850981977923\n",
      "d41: 0.003403859936393694\n",
      "d38: 0.003328749025149902\n",
      "d23: 0.0033012922545496703\n",
      "d29: 0.0032881666490300887\n",
      "n39: 0.003246980273988731\n",
      "d59: 0.003245075965748745\n",
      "d67: 0.00324240890628702\n",
      "d27: 0.00323127786176447\n",
      "d18: 0.00321783190572833\n",
      "d85: 0.0031499586712950833\n",
      "d47: 0.0031261901529702207\n",
      "a3: 0.0031145768981319932\n",
      "d7: 0.0031009783971083227\n",
      "n37: 0.003065740835876518\n",
      "a12: 0.0030428226626135757\n",
      "n1: 0.0030404085533803915\n",
      "d6: 0.003038587516997593\n",
      "n90: 0.0029456224756936322\n",
      "d54: 0.002927890557354207\n",
      "a5: 0.002910862447241813\n",
      "d1: 0.002883591536149366\n",
      "n15: 0.002834030737561166\n",
      "n47: 0.0028128408729863885\n",
      "d81: 0.0027820977032519956\n",
      "n0: 0.0027332385365905765\n",
      "d95: 0.0026621474843706583\n",
      "n97: 0.002632014366019453\n",
      "n84: 0.0026039621010198744\n",
      "d35: 0.002602644544142052\n",
      "n22: 0.002569653290038554\n",
      "n29: 0.0025361648676063275\n",
      "d32: 0.0025228708982643067\n",
      "n61: 0.0025151550091477848\n",
      "a18: 0.002512376043244391\n",
      "n65: 0.0025106517270410134\n",
      "d78: 0.002441955177011179\n",
      "n21: 0.002427585297585688\n",
      "d94: 0.0024008290344328387\n",
      "n19: 0.002358831388731941\n",
      "n11: 0.00235698047690569\n",
      "n82: 0.002221747893814019\n",
      "n30: 0.0022042753506859025\n",
      "n28: 0.0021657233124932063\n",
      "d93: 0.002139081383559116\n",
      "n93: 0.0021201606657006433\n",
      "d20: 0.002074963464076207\n",
      "d75: 0.002030492650816784\n",
      "n26: 0.0019963465305041783\n",
      "n74: 0.0019742634765671507\n",
      "d58: 0.001947847655959789\n",
      "n71: 0.001928784835257158\n",
      "d37: 0.0019147132207228967\n",
      "n67: 0.001880271288412505\n",
      "n6: 0.0017062716529223643\n",
      "n86: 0.0016622664777314888\n",
      "d26: 0.0015686165396078255\n",
      "a11: 0.001544752286689599\n",
      "d61: 0.001519430201678329\n",
      "d16: 0.0015117420096297796\n",
      "d4: 0.0015115777025920263\n",
      "d73: 0.0015101716298240309\n",
      "d36: 0.001497468805413238\n",
      "d63: 0.0014961015210865902\n",
      "d3: 0.001488048478192061\n",
      "n36: 0.0014830595668400282\n",
      "n66: 0.0014422631001385078\n",
      "d11: 0.0014200571495766035\n",
      "a2: 0.0014144163923506259\n",
      "d74: 0.00141321661321836\n",
      "n64: 0.0013653259149899455\n",
      "a15: 0.0013383377251547923\n",
      "n42: 0.0012884487437980763\n",
      "d60: 0.0012842638228700576\n",
      "d91: 0.0012590403287160434\n",
      "a1: 0.0012218380623278957\n",
      "a8: 0.0012174877219008895\n",
      "n7: 0.0010790098411750382\n",
      "d22: 0.0010563722462701985\n",
      "a4: 0.0010114622009711471\n",
      "d13: 0.0010012363630287346\n",
      "d65: 0.0008949125158439486\n",
      "d21: 0.0008745254921329604\n",
      "n56: 0.0008598748809016676\n",
      "d71: 0.0008367371153512604\n",
      "n16: 0.0008327287864264843\n",
      "d40: 0.0008286980037386371\n",
      "d8: 0.000824055981853089\n",
      "a13: 0.0008205449985223545\n",
      "d50: 0.0008032203352248413\n",
      "d51: 0.000759553086078224\n",
      "n9: 0.0006407297031667536\n",
      "n99: 0.0006306969509186633\n",
      "d25: 0.0006157086273765877\n",
      "d28: 0.0005728627314751655\n",
      "n76: 0.000565287257586311\n",
      "n10: 0.0005610186012949825\n",
      "n24: 0.00052784213530499\n",
      "d98: 0.00028953152209632016\n",
      "d64: 0.0001459460312536187\n",
      "d99: 8.707572251509532e-05\n",
      "d19: 1.4543814768286367e-05\n",
      "n2: 0.0\n",
      "n3: 0.0\n",
      "n12: 0.0\n",
      "n13: 0.0\n",
      "n17: 0.0\n",
      "n35: 0.0\n",
      "n46: 0.0\n",
      "n51: 0.0\n",
      "n52: 0.0\n",
      "n53: 0.0\n",
      "n54: 0.0\n",
      "n57: 0.0\n",
      "n62: 0.0\n",
      "n68: 0.0\n",
      "n69: 0.0\n",
      "n73: 0.0\n",
      "n75: 0.0\n",
      "n77: 0.0\n",
      "n79: 0.0\n",
      "n80: 0.0\n",
      "n83: 0.0\n",
      "n87: 0.0\n",
      "n89: 0.0\n",
      "n92: 0.0\n",
      "n94: 0.0\n",
      "a0: 0.0\n",
      "a7: 0.0\n",
      "a9: 0.0\n",
      "a10: 0.0\n",
      "a14: 0.0\n",
      "a17: 0.0\n",
      "a19: 0.0\n",
      "d0: 0.0\n",
      "d10: 0.0\n",
      "d14: 0.0\n",
      "d30: 0.0\n",
      "d39: 0.0\n",
      "d42: 0.0\n",
      "d49: 0.0\n",
      "d56: 0.0\n",
      "d57: 0.0\n",
      "d68: 0.0\n",
      "d69: 0.0\n",
      "d70: 0.0\n",
      "d72: 0.0\n",
      "d76: 0.0\n",
      "d82: 0.0\n",
      "d83: 0.0\n",
      "d84: 0.0\n",
      "d89: 0.0\n",
      "d90: 0.0\n",
      "d97: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset into a pandas DataFrame\n",
    "\n",
    "# Select the continuous features you want to discretize\n",
    "continuous_features = list(combined_df.drop('rating_label', axis=1).columns)\n",
    "\n",
    "# Discretize the continuous features using equal width binning\n",
    "n_bins = 5  # Number of bins\n",
    "discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "discretized_data = discretizer.fit_transform(combined_df[continuous_features])\n",
    "\n",
    "# Create a new DataFrame with discretized features\n",
    "df_discretized = pd.DataFrame(discretized_data, columns=continuous_features)\n",
    "\n",
    "# Perform feature selection using mutual information\n",
    "target_variable = 'rating_label'  # Your target variable\n",
    "X = df_discretized  # Features\n",
    "y = combined_df[target_variable]  # Target variable\n",
    "\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "selected_features = selector.fit_transform(X, y)\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "selected_feature_names = [X.columns[idx] for idx in selected_feature_indices]\n",
    "selected_feature_scores = selector.scores_[selected_feature_indices]\n",
    "\n",
    "# Sort features by score in descending order\n",
    "sorted_features = sorted(zip(selected_feature_names, selected_feature_scores),\n",
    "                         key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the selected feature names and their mutual information scores in order\n",
    "print(\"Selected Features and Mutual Information Scores (in descending order):\")\n",
    "for feature, score in sorted_features:\n",
    "    print(f\"{feature}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04aeed94-5181-43bb-b6ce-5513be8c2190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score for d-features: 0.0028322501282761745\n",
      "Average score for n-features: 0.002405192864826591\n",
      "Average score for a-features: 0.002568532555710723\n"
     ]
    }
   ],
   "source": [
    "# Calculate averages for each letter-group\n",
    "averages = {}\n",
    "for feature, score in sorted_features:\n",
    "    letter_group = feature[0]  # Get the first character of the feature name\n",
    "    if letter_group not in averages:\n",
    "        averages[letter_group] = []\n",
    "    averages[letter_group].append(score)\n",
    "\n",
    "# Compute the average for each letter-group\n",
    "for letter_group, scores in averages.items():\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    print(f\"Average score for {letter_group}-features: {average_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532cfdf0-a2bb-44a9-ad79-388e0f584e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
