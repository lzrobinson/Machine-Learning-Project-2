{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c297293b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25584\\2846189776.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0md2v\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[0mdoc2vecLangs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlanguageClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data/book_rating_train.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Data/book_text_features_doc2vec/train_desc_doc2vec100.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25584\\2846189776.py\u001b[0m in \u001b[0;36mlanguageClassifier\u001b[1;34m(filename, doc2vec)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#loading data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0md2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc2vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print(d2v)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def languageClassifier(filename, doc2vec):\n",
    "    \n",
    "    #loading data\n",
    "    data = pd.read_csv(filename)\n",
    "    d2v = pd.read_csv(doc2vec, index_col = False, delimiter = ',', header = None)\n",
    "    #print(d2v)\n",
    "    \n",
    "    \n",
    "    #combining doc2vec and language into a df\n",
    "    languages = data[\"Language\"]\n",
    "    #print(d2v.shape)\n",
    "   # print(languages)\n",
    "    d2v[\"Language\"] = languages\n",
    "    #print(d2v)\n",
    "    count = 0\n",
    "\n",
    "    \n",
    "    #sorting values by language, pruning NaNs\n",
    "    knownLanguage = d2v.dropna()\n",
    "    languages = languages.dropna()\n",
    "    \n",
    "    \n",
    "    #sorting remaining instances by language\n",
    "    data_by_lang = [(x,y.iloc[:,:]) for x, y in knownLanguage.groupby(languages)]\n",
    "    \n",
    "    #calculating centroids of each language\n",
    "    langCentroids = []\n",
    "    for language, df in data_by_lang:\n",
    "        df.pop(\"Language\")\n",
    "        df.loc[language] = df.mean()\n",
    "        df = df.iloc[-1]\n",
    "        \n",
    "        langCentroids.append(df)\n",
    "        \n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "   \n",
    "    while i < len(d2v.index):\n",
    "        \n",
    "        if pd.isna(d2v.at[i, \"Language\"]):\n",
    "            \n",
    "            currRow = d2v.iloc[i].iloc[:-1]\n",
    "            \n",
    "            bestLang = langClass(langCentroids, currRow)\n",
    "            d2v.at[i, \"Language\"] = bestLang\n",
    "        i += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    bestLang = langClass(langCentroids, currRow)\n",
    "    #print(currRow)\n",
    "   # print(langCentroids)\n",
    "    print(bestLang)\n",
    "    print(d2v)\n",
    "   \n",
    "    return d2v\n",
    "\n",
    "doc2vecLangs = languageClassifier(\"Data/book_rating_train.csv\", \"Data/book_text_features_doc2vec/train_desc_doc2vec100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd80537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################distance and similarity measures used ################\n",
    "\n",
    "def cosine_sim(v1, v2):\n",
    "    #note v1 and v2 are pandas series of equal length\n",
    "    \n",
    "    i = 0\n",
    "    dotProduct = 0\n",
    "    absV1 = 0\n",
    "    absV2 = 0\n",
    "    \n",
    "    while i < len(v1):\n",
    "        dotProduct += v1[i]*v2[i]\n",
    "        absV1 += v1[i]**2\n",
    "        absV2 += v2[i]**2\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "    absV1 = np.sqrt(absV1)\n",
    "    absV2 = np.sqrt(absV2)\n",
    "    return dotProduct / (absV1 * absV2)\n",
    "\n",
    "def manDistance(v1, v2):\n",
    "    \n",
    "    i = 0\n",
    "    totalDistance = 0\n",
    "    while i < len(v1):\n",
    "        totalDistance = np.abs(v1[i] - v2[i])\n",
    "        i += 1\n",
    "        \n",
    "    return totalDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75619216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for finding the best language class, first is more robust\n",
    "\n",
    "def langClass(centroids, row, mode = \"cosine\"):\n",
    "    \n",
    "    simScores = {}\n",
    "    print(row.name)\n",
    "    for centroid in centroids:\n",
    "        if mode == \"cosine\":\n",
    "            simScores[centroid.name] = cosine_sim(centroid, row)\n",
    "        else:\n",
    "            simScores[centroid.name] = manDistance(centroid, row)\n",
    "        \n",
    "    #setting eng as base lang\n",
    "    maxSim = simScores[\"fre\"]\n",
    "    bestLang = \"fre\"\n",
    "    for lang, sim in simScores.items():\n",
    "        \n",
    "        #print(f\"lang is {lang}, sim is {sim}\")\n",
    "        \n",
    "        if mode == \"cosine\":\n",
    "            if sim > maxSim:\n",
    "                maxSim = sim\n",
    "                bestLang = lang\n",
    "                \n",
    "        else:\n",
    "            if sim < maxSim:\n",
    "                maxSim = sim\n",
    "                bestLang = lang\n",
    "    return bestLang\n",
    "\n",
    "\n",
    "def langClass2(centroids, row):\n",
    "    \n",
    "    simScores = {}\n",
    "    for centroid in centroids:\n",
    "         simScores[centroid.name] = manDistance(centroid, row)\n",
    "        \n",
    "    #setting eng as base lang\n",
    "    maxSim = simScores[\"fre\"]\n",
    "    bestLang = \"fre\"\n",
    "    for lang, sim in simScores.items():\n",
    "        \n",
    "        #print(f\"lang is {lang}, sim is {sim}\")\n",
    "    \n",
    "        if sim < maxSim:\n",
    "            maxSim = sim\n",
    "            bestLang = lang\n",
    "            \n",
    "    return bestLang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217dbc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Counts with Cosine Similarity\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'manDistClass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25584\\2734545475.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Language Counts with Cosine Similarity\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmanDistClass\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Language\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nLanguage Counts with Manhattan Distance\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'manDistClass' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Language Counts with Cosine Similarity\")\n",
    "print(manDistClass[\"Language\"].value_counts())\n",
    "\n",
    "print(\"\\nLanguage Counts with Manhattan Distance\")\n",
    "\n",
    "print(manDist2Class[\"Language\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ac6d0f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25584\\4293023253.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[0mmanDist2Class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlanguageClassifier2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data/book_rating_train.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Data/book_text_features_doc2vec/train_desc_doc2vec100.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25584\\4293023253.py\u001b[0m in \u001b[0;36mlanguageClassifier2\u001b[1;34m(filename, doc2vec)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#loading data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0md2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc2vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#print(d2v)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "'''Same as previous one, but using Manhattan distance, very slow,least reliable'''\n",
    "\n",
    "def languageClassifier2(filename, doc2vec):\n",
    "    \n",
    "    #loading data\n",
    "    data = pd.read_csv(filename)\n",
    "    d2v = pd.read_csv(doc2vec, index_col = False, delimiter = ',', header = None)\n",
    "    #print(d2v)\n",
    "    \n",
    "    \n",
    "    #combining doc2vec and language into a df\n",
    "    languages = data[\"Language\"]\n",
    "    #print(d2v.shape)\n",
    "   # print(languages)\n",
    "    d2v[\"Language\"] = languages\n",
    "    #print(d2v)\n",
    "    count = 0\n",
    "\n",
    "    \n",
    "    #sorting values by language, pruning NaNs\n",
    "    knownLanguage = d2v.dropna()\n",
    "    languages = languages.dropna()\n",
    "    \n",
    "    \n",
    "    #sorting remaining instances by language\n",
    "    data_by_lang = [(x,y.iloc[:,:]) for x, y in knownLanguage.groupby(languages)]\n",
    "    \n",
    "    #calculating centroids of each language\n",
    "    langCentroids = []\n",
    "    for language, df in data_by_lang:\n",
    "        df.pop(\"Language\")\n",
    "        df.loc[language] = df.mean()\n",
    "        df = df.iloc[-1]\n",
    "        \n",
    "        langCentroids.append(df)\n",
    "        \n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "   \n",
    "    while i < len(d2v.index):\n",
    "        \n",
    "        if pd.isna(d2v.at[i, \"Language\"]):\n",
    "            \n",
    "            currRow = d2v.iloc[i].iloc[:-1]\n",
    "            \n",
    "            bestLang = langClass2(langCentroids, currRow)\n",
    "            d2v.at[i, \"Language\"] = bestLang\n",
    "        i += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(currRow)\n",
    "   # print(langCentroids)\n",
    "    print(bestLang)\n",
    "    print(d2v)\n",
    "   \n",
    "    return d2v\n",
    "\n",
    "\n",
    "manDist2Class = languageClassifier2(\"Data/book_rating_train.csv\", \"Data/book_text_features_doc2vec/train_desc_doc2vec100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Uses Cosine similarity as metric, bad, takes too long, seems not good'''\n",
    "def languageClassifier(filename, doc2vec):\n",
    "    \n",
    "    #loading data\n",
    "    data = pd.read_csv(filename)\n",
    "    d2v = pd.read_csv(doc2vec, index_col = False, delimiter = ',', header = None)\n",
    "    #print(d2v)\n",
    "    \n",
    "    \n",
    "    #combining doc2vec and language into a df\n",
    "    languages = data[\"Language\"]\n",
    "    #print(d2v.shape)\n",
    "   # print(languages)\n",
    "    d2v[\"Language\"] = languages\n",
    "    #print(d2v)\n",
    "    count = 0\n",
    "\n",
    "    \n",
    "    #sorting values by language, pruning NaNs\n",
    "    knownLanguage = d2v.dropna()\n",
    "    languages = languages.dropna()\n",
    "    \n",
    "    \n",
    "    #sorting remaining instances by language\n",
    "    data_by_lang = [(x,y.iloc[:,:]) for x, y in knownLanguage.groupby(languages)]\n",
    "    \n",
    "    #calculating centroids of each language\n",
    "    langCentroids = []\n",
    "    for language, df in data_by_lang:\n",
    "        df.pop(\"Language\")\n",
    "        df.loc[language] = df.mean()\n",
    "        df = df.iloc[-1]\n",
    "        \n",
    "        langCentroids.append(df)\n",
    "        \n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "   \n",
    "    while i < len(d2v.index):\n",
    "        \n",
    "        if pd.isna(d2v.at[i, \"Language\"]):\n",
    "            \n",
    "            currRow = d2v.iloc[i].iloc[:-1]\n",
    "            \n",
    "            bestLang = langClass(langCentroids, currRow)\n",
    "            d2v.at[i, \"Language\"] = bestLang\n",
    "        i += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    bestLang = langClass(langCentroids, currRow)\n",
    "    #print(currRow)\n",
    "   # print(langCentroids)\n",
    "    print(bestLang)\n",
    "    print(d2v)\n",
    "   \n",
    "    return d2v\n",
    "\n",
    "doc2vecLangs = languageClassifier(\"Data/book_rating_train.csv\", \"Data/book_text_features_doc2vec/train_desc_doc2vec100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef0eaf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "'''Seems to be a good model, has good counts, verified a few instances seems to perform great in fact'''\n",
    "def learnLanguage(file, doc2vec):\n",
    "    \n",
    "    \n",
    "    data = pd.read_csv(file)\n",
    "    X = pd.read_csv(doc2vec, index_col = False, delimiter = ',', header = None)\n",
    "    y = data[\"Language\"]\n",
    "    \n",
    "    y = pd.Series(y)\n",
    "    # Assuming you have a feature matrix `X` and a target variable `y`\n",
    "    # X should contain other features like doc2vec and word frequency counts\n",
    "    # y should contain the language labels (with missing values)\n",
    "\n",
    "    # Split the dataset into instances with and without missing language values\n",
    "    X_with_language = X[~y.isnull()]\n",
    "    y_with_language = y[~y.isnull()]\n",
    "    X_missing_language = X[y.isnull()]\n",
    "\n",
    "    # Split the dataset with language into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_with_language, y_with_language, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train a Random Forest classifier on the instances with language\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the missing language values using the trained model\n",
    "    imputed_language = rf.predict(X_missing_language)\n",
    "\n",
    "    # Merge the imputed language values with the original dataset\n",
    "    y[y.isnull()] = imputed_language\n",
    "\n",
    "    # Now you can proceed with your machine learning algorithm using the complete dataset\n",
    "    return y\n",
    "\n",
    "lol = learnLanguage(\"Data/book_rating_train.csv\", \"Data/book_text_features_doc2vec/train_desc_doc2vec100.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21d937c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language counts with Random-Forest classifier\n",
      "eng    22555\n",
      "spa      217\n",
      "fre      175\n",
      "ger       65\n",
      "jpn       10\n",
      "per        8\n",
      "mul        7\n",
      "por        5\n",
      "lat        4\n",
      "ita        4\n",
      "zho        3\n",
      "grc        2\n",
      "heb        2\n",
      "rus        2\n",
      "ara        1\n",
      "swe        1\n",
      "frs        1\n",
      "nld        1\n",
      "Name: Language, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Language counts with Random-Forest classifier\")\n",
    "print(lol.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50988496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pd.option_context('display.max_rows', None,\n",
    "#                       'display.max_columns', None,\n",
    "#                       'display.precision', 3,\n",
    "#                       ):\n",
    "#    display(lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958a952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
